### 신경망 이해하기
- 비선형 문제는 $$b+w_{1} x_{1}+w_{2} x_{2}$$의 형태로 모델을 예측할 수 없음
	- 다시 말해 Decision Boundary가 선이 아님
- 복잡한 비선형 문제는 선형 모델로 해결할 수 없음
- 히든 레이어를 추가해 출력을 설명하려고 함
- 활성화 함수
	- 비선형 문제를 모델링하기 위해 비선형성을 직접 대입
	- 히든 레이어의 노드가 비선형 함수를 통과하도록 할 수 있음
	- 아래 그림의 그래프로 나타낸 모델에서 히든 레이어 1의 각 노드 값이 비선형 함수로 변환된 후에 다음 레이어의 가중 합으로 전달됨
	- <img src="https://developers.google.com/machine-learning/crash-course/images/activation.svg">
	- 이 비선형 함수를 활성화 함수라 함
- 일반적인 활성화 함수
	- 시그모이드
	- 가중 합을 0~1 사이로 변환
- ReLU
	- 효과적이고 쉽게 계산 가능


	
### 신경망 학습 이해하기
- 역전파에서 생길 수 있는 문제
	- Gradient Descent
	- Gradient 발산
	- ReLU unit 소멸
- Drop out 정규화


### 다중 클래스 신경망
- 일대다는 이진 분류 활용 방법을 제공
- 소프트맥스
	
	```
		전체 소프트맥스는 지금까지 설명한 소프트맥스입니다. 즉 모든 가능한 클래스의 확률을 계산하는 소프트맥스입니다.
	
	후보 샘플링은 소프트맥스가 양성 라벨의 확률은 모두 계산하지만 음성 라벨의 경우 무작위 샘플의 확률만 계산합니다. 예를 들어 입력 이미지가 비글인지, 블러드하운드인지 확인하는 경우 개가 아닌 예의 확률은 제공할 필요가 없습니다.
	
	전체 소프트맥스는 클래스 수가 적으면 매우 적은 비용이 들지만 클래스 수가 증가하면 엄청나게 많은 비용이 듭니다. 클래스 수가 많은 문제에서 후보 샘플링을 사용하면 효율성이 향상됩니다.
	
	라벨 1개 대 라벨 여러 개
	소프트맥스에서는 각 예가 정확히 한 클래스의 멤버라고 가정합니다. 하지만 동시에 여러 클래스의 멤버인 예도 있습니다. 이러한 예의 경우
	
	소프트맥스를 사용할 수 없습니다.
	로지스틱 회귀를 여러 개 사용해야 합니다
	```