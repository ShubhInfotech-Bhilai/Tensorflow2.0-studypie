### L2 정규화
- 일반화 곡선은 학습 반복 횟수에 대해 학습 세트와 검증 세트의 loss를 보여줌


- Train loss는 점점 감소하지만 Test loss가 감소하다 증가하는 경우
- Train 데이터에 오버피팅
- 정규화를 사용해 해결
- 손실 + 복잡성을 최소화해 structural risk minimization
- 모델 복잡도를 2가지 방법으로 다룸
	- 모든 feature 가중치
	- 0이 아닌 가중치를 사용하는 feature의 총 개수 
- 가중치 함수를 사용해 모델 복잡도를 볼 경우, 절대값이 높은 feature 가중치는 낮은 값보다 복잡
- 정규화 항을 모든 feature의 가중치 제곱의 합으로 정의하는 것이 L2
	- 가중치가 0에 가까우면 모델의 복잡도에 영향을 미치지 않지만 이상치에 영향을 줌

	
### Lambda
- 정규화율을 표시
- 정규화항에 곱해 전반적인 영향을 조정함
	- 가중치 값을 0으로 유도
	- 정규 분포를 사용해 가중치 평균을 0으로 유도
- 람다값이 너무 높으면 모델은 단순해지지만 언더피팅의 가능성이 있음
- 람다값이 너무 낮으면 모델은 더 복잡해지고 데이터가 과적합해질 위험이 있음	


### 로지스틱 회귀
- 0 또는 1을 예측하는 대신 확률을 생성함
- 로지스틱 회귀는 확률 계산 메커니즘
- 반환된 확률을 사용하는 방법
	- 있는 그대로
	- 이진 카테고리 변환
- 한 밤중에 개가 짖는 확률 => P(개가 짖는다|밤에)
	- 이 값이 0.05면 1년동안 약 18번 깸
- 많은 경우 로지스틱 회귀를 이진 분류 문제 해결 방법으로 적용함
- 0 ~ 1 사이에 포함되도록 시그모이드 함수를 사용
	- 시그모이드는 0과 1 사이의 값을 생성함
- Loss
	- 로그 loss
		- <img src="https://www.dropbox.com/s/5fx71n7xdhe57wv/Screenshot%202019-12-14%2000.04.12.png?raw=1">
	- log loss 식은 엔트로피와 밀접한 관련이 있음
	- 우도 함수의 음의 로그로 y의 베르누이 분포를 가정함
	- 손실 함수를 최소화하면 최대 우도 추정치가 생성됨
- 로지스틱 회귀의 정규화
	- 정규화는 L2 또는 조기 중단으로 사용함
	- L1은 추후 설명
-    